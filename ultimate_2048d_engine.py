"""
ultimate_2048d_engine.py - Ultimate 2048D Face Recognition Engine
"KWIKPIC ANNIHILATOR" - The most advanced face recognition system ever built

This is a separate engine from the 512D system, providing fallback options.
"""

import numpy as np
import os
import cv2
from typing import List, Dict, Any, Optional, Tuple
import torch
import torch.nn as nn
from sklearn.preprocessing import normalize
from sklearn.metrics.pairwise import cosine_similarity

# Try to import advanced libraries
try:
    from facenet_pytorch import MTCNN, InceptionResnetV1
    _HAS_FACENET = True
    print("‚úÖ FaceNet available for 2048D engine")
except ImportError:
    _HAS_FACENET = False
    print("‚ö†Ô∏è FaceNet not available, using fallback models")

try:
    import insightface
    from insightface.app import FaceAnalysis
    _HAS_INSIGHTFACE = True
    print("‚úÖ InsightFace available for 2048D engine")
except ImportError:
    _HAS_INSIGHTFACE = False
    print("‚ö†Ô∏è InsightFace not available, using fallback models")

class Ultimate2048DDetector:
    """
    Ultimate 2048D Face Detector - The Kwikpic Annihilator
    Combines multiple models to create 2048D super-embeddings
    """
    
    def __init__(self):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.models = {}
        self.embeddings_dim = 2048
        self._initialize_models()
        
    def _initialize_models(self):
        """Initialize all available models for 2048D ensemble"""
        print("üî• Initializing ULTIMATE 2048D MODELS...")
        
        # Model 1: MTCNN + FaceNet (512D ‚Üí 256D)
        if _HAS_FACENET:
            try:
                self.models['mtcnn'] = MTCNN(
                    image_size=160,
                    margin=0,
                    min_face_size=20,
                    thresholds=[0.6, 0.7, 0.7],
                    factor=0.709,
                    post_process=True,
                    device=self.device
                )
                
                self.models['facenet'] = InceptionResnetV1(pretrained='vggface2').eval()
                if self.device == 'cuda':
                    self.models['facenet'] = self.models['facenet'].cuda()
                
                print("‚úÖ Model 1: MTCNN + FaceNet loaded (512D ‚Üí 256D)")
            except Exception as e:
                print(f"‚ö†Ô∏è Model 1 failed: {e}")
        
        # Model 2: InsightFace (512D ‚Üí 256D)
        if _HAS_INSIGHTFACE:
            try:
                self.models['insightface'] = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])
                self.models['insightface'].prepare(ctx_id=0, det_size=(640, 640))
                print("‚úÖ Model 2: InsightFace loaded (512D ‚Üí 256D)")
            except Exception as e:
                print(f"‚ö†Ô∏è Model 2 failed: {e}")
        
        # Model 3: Custom 256D Expander
        self.models['expander'] = self._create_256d_expander()
        print("‚úÖ Model 3: 256D Expander created")
        
        # Model 4: Histogram-based 256D
        self.models['histogram'] = self._create_histogram_extractor()
        print("‚úÖ Model 4: Histogram Extractor created")
        
        # Model 5: Edge-based 256D
        self.models['edge'] = self._create_edge_extractor()
        print("‚úÖ Model 5: Edge Extractor created")
        
        # Model 6: Color-based 256D
        self.models['color'] = self._create_color_extractor()
        print("‚úÖ Model 6: Color Extractor created")
        
        # Model 7: Texture-based 256D
        self.models['texture'] = self._create_texture_extractor()
        print("‚úÖ Model 7: Texture Extractor created")
        
        # Model 8: Frequency-based 256D
        self.models['frequency'] = self._create_frequency_extractor()
        print("‚úÖ Model 8: Frequency Extractor created")
        
        print(f"üî• {len(self.models)} MODELS READY FOR 2048D ENSEMBLE!")
    
    def _create_256d_expander(self):
        """Create a simple function to expand 512D to 256D (no neural network for now)"""
        def expand_512d_to_256d(embedding):
            """Simple expansion: take first 256D or pad/truncate"""
            if len(embedding) >= 256:
                return embedding[:256]
            else:
                return np.pad(embedding, (0, 256 - len(embedding)), mode='constant')
        
        return expand_512d_to_256d
    
    def _create_histogram_extractor(self):
        """Create histogram-based feature extractor"""
        def extract_histogram_features(image):
            # Convert to different color spaces
            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            
            # Extract histograms
            hist_rgb = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
            hist_hsv = cv2.calcHist([hsv], [0, 1, 2], None, [8, 8, 8], [0, 180, 0, 256, 0, 256])
            hist_lab = cv2.calcHist([lab], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
            
            # Flatten and normalize
            features = np.concatenate([
                hist_rgb.flatten(),
                hist_hsv.flatten(),
                hist_lab.flatten()
            ])
            
            # Pad or truncate to 256D
            if len(features) > 256:
                features = features[:256]
            else:
                features = np.pad(features, (0, 256 - len(features)), mode='constant')
            
            return features.astype(np.float32)
        
        return extract_histogram_features
    
    def _create_edge_extractor(self):
        """Create edge-based feature extractor"""
        def extract_edge_features(image):
            # Convert to grayscale
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # Apply different edge detectors
            sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            laplacian = cv2.Laplacian(gray, cv2.CV_64F)
            canny = cv2.Canny(gray, 50, 150)
            
            # Extract features from each
            features = []
            for edge_img in [sobel_x, sobel_y, laplacian, canny]:
                # Calculate statistics
                features.extend([
                    np.mean(edge_img),
                    np.std(edge_img),
                    np.var(edge_img),
                    np.median(edge_img),
                    np.percentile(edge_img, 25),
                    np.percentile(edge_img, 75)
                ])
            
            # Pad to 256D
            features = np.array(features)
            if len(features) > 256:
                features = features[:256]
            else:
                features = np.pad(features, (0, 256 - len(features)), mode='constant')
            
            return features.astype(np.float32)
        
        return extract_edge_features
    
    def _create_color_extractor(self):
        """Create color-based feature extractor"""
        def extract_color_features(image):
            # Convert to different color spaces
            hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
            yuv = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)
            
            features = []
            for color_space in [image, hsv, lab, yuv]:
                for channel in range(3):
                    channel_data = color_space[:, :, channel]
                    features.extend([
                        np.mean(channel_data),
                        np.std(channel_data),
                        np.var(channel_data),
                        np.median(channel_data),
                        np.percentile(channel_data, 10),
                        np.percentile(channel_data, 90),
                        np.percentile(channel_data, 25),
                        np.percentile(channel_data, 75)
                    ])
            
            # Pad to 256D
            features = np.array(features)
            if len(features) > 256:
                features = features[:256]
            else:
                features = np.pad(features, (0, 256 - len(features)), mode='constant')
            
            return features.astype(np.float32)
        
        return extract_color_features
    
    def _create_texture_extractor(self):
        """Create texture-based feature extractor"""
        def extract_texture_features(image):
            # Convert to grayscale
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # Apply Gabor filters
            features = []
            for theta in [0, 45, 90, 135]:
                for frequency in [0.1, 0.3, 0.5]:
                    kernel = cv2.getGaborKernel((21, 21), 5, theta, 10, 0.5, 0, ktype=cv2.CV_32F)
                    filtered = cv2.filter2D(gray, cv2.CV_8UC3, kernel)
                    features.extend([
                        np.mean(filtered),
                        np.std(filtered),
                        np.var(filtered)
                    ])
            
            # Local Binary Pattern (if available)
            try:
                from skimage.feature import local_binary_pattern
                lbp = local_binary_pattern(gray, 8, 1, method='uniform')
                hist_lbp, _ = np.histogram(lbp.ravel(), bins=10, range=(0, 10))
                features.extend(hist_lbp)
            except ImportError:
                # Fallback: simple texture features
                features.extend([0] * 10)  # Add zeros for missing LBP features
            
            # Pad to 256D
            features = np.array(features)
            if len(features) > 256:
                features = features[:256]
            else:
                features = np.pad(features, (0, 256 - len(features)), mode='constant')
            
            return features.astype(np.float32)
        
        return extract_texture_features
    
    def _create_frequency_extractor(self):
        """Create frequency-based feature extractor"""
        def extract_frequency_features(image):
            # Convert to grayscale
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # Apply FFT
            fft = np.fft.fft2(gray)
            fft_shift = np.fft.fftshift(fft)
            magnitude = np.abs(fft_shift)
            phase = np.angle(fft_shift)
            
            # Extract features from frequency domain
            features = []
            
            # Magnitude features
            features.extend([
                np.mean(magnitude),
                np.std(magnitude),
                np.var(magnitude),
                np.median(magnitude),
                np.percentile(magnitude, 25),
                np.percentile(magnitude, 75)
            ])
            
            # Phase features
            features.extend([
                np.mean(phase),
                np.std(phase),
                np.var(phase),
                np.median(phase),
                np.percentile(phase, 25),
                np.percentile(phase, 75)
            ])
            
            # Frequency bands
            h, w = magnitude.shape
            center_h, center_w = h // 2, w // 2
            
            # Low frequency
            low_freq = magnitude[center_h-10:center_h+10, center_w-10:center_w+10]
            features.extend([np.mean(low_freq), np.std(low_freq)])
            
            # High frequency
            high_freq = magnitude[0:10, 0:10]
            features.extend([np.mean(high_freq), np.std(high_freq)])
            
            # Pad to 256D
            features = np.array(features)
            if len(features) > 256:
                features = features[:256]
            else:
                features = np.pad(features, (0, 256 - len(features)), mode='constant')
            
            return features.astype(np.float32)
        
        return extract_frequency_features
    
    def detect_faces_2048d(self, image_path: str) -> List[np.ndarray]:
        """
        Detect faces and return 2048D embeddings
        """
        try:
            # Load image
            if isinstance(image_path, str):
                image = cv2.imread(image_path)
                if image is None:
                    return []
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            else:
                image_rgb = image_path
            
            # Extract 256D features from each model
            model_features = {}
            
            # Model 1: MTCNN + FaceNet
            if 'mtcnn' in self.models and 'facenet' in self.models:
                try:
                    mtcnn_features = self._extract_mtcnn_facenet_features(image_rgb)
                    model_features['mtcnn_facenet'] = mtcnn_features
                except Exception as e:
                    print(f"‚ö†Ô∏è MTCNN+FaceNet failed: {e}")
            
            # Model 2: InsightFace
            if 'insightface' in self.models:
                try:
                    insightface_features = self._extract_insightface_features(image_rgb)
                    model_features['insightface'] = insightface_features
                except Exception as e:
                    print(f"‚ö†Ô∏è InsightFace failed: {e}")
            
            # Models 3-8: Custom extractors
            for model_name, extractor in self.models.items():
                if model_name in ['expander', 'histogram', 'edge', 'color', 'texture', 'frequency']:
                    try:
                        features = extractor(image_rgb)
                        model_features[model_name] = features
                    except Exception as e:
                        print(f"‚ö†Ô∏è {model_name} failed: {e}")
            
            # Combine all features into 2048D
            if not model_features:
                print("‚ö†Ô∏è No models succeeded, returning empty")
                return []
            
            # Ensure we have 8 x 256D = 2048D
            combined_features = []
            for i in range(8):
                model_key = list(model_features.keys())[i % len(model_features)]
                features = model_features[model_key]
                
                # Ensure 256D
                if len(features) > 256:
                    features = features[:256]
                else:
                    features = np.pad(features, (0, 256 - len(features)), mode='constant')
                
                combined_features.extend(features)
            
            # Final 2048D embedding
            final_embedding = np.array(combined_features, dtype=np.float32)
            
            # Normalize
            final_embedding = normalize(final_embedding.reshape(1, -1)).flatten()
            
            print(f"‚úÖ Generated 2048D embedding: {len(final_embedding)} dimensions")
            return [final_embedding]
            
        except Exception as e:
            print(f"‚ùå 2048D detection failed: {e}")
            return []
    
    def _extract_mtcnn_facenet_features(self, image_rgb):
        """Extract features using MTCNN + FaceNet"""
        # Detect faces
        boxes, probs, landmarks = self.models['mtcnn'].detect(image_rgb, landmarks=True)
        
        if boxes is None or len(boxes) == 0:
            return np.zeros(256, dtype=np.float32)
        
        # Get the best face
        best_idx = np.argmax(probs)
        if probs[best_idx] < 0.9:
            return np.zeros(256, dtype=np.float32)
        
        # Extract face
        x1, y1, x2, y2 = boxes[best_idx].astype(int)
        face = image_rgb[y1:y2, x1:x2]
        
        # Get FaceNet embedding
        face_tensor = self.models['mtcnn'](face)
        if face_tensor is not None:
            with torch.no_grad():
                embedding = self.models['facenet'](face_tensor.unsqueeze(0))
                embedding = embedding.cpu().numpy().flatten()
                
                # Expand 512D to 256D using expander
                if 'expander' in self.models:
                    return self.models['expander'](embedding)
                else:
                    # Simple truncation
                    return embedding[:256] if len(embedding) > 256 else np.pad(embedding, (0, 256 - len(embedding)), mode='constant')
        
        return np.zeros(256, dtype=np.float32)
    
    def _extract_insightface_features(self, image_rgb):
        """Extract features using InsightFace"""
        try:
            faces = self.models['insightface'].get(image_rgb)
            if not faces:
                return np.zeros(256, dtype=np.float32)
            
            # Get the best face
            best_face = max(faces, key=lambda x: x.bbox[2] * x.bbox[3])
            embedding = best_face.embedding
            
            # Expand 512D to 256D
            if len(embedding) > 256:
                return embedding[:256]
            else:
                return np.pad(embedding, (0, 256 - len(embedding)), mode='constant')
                
        except Exception as e:
            print(f"‚ö†Ô∏è InsightFace extraction failed: {e}")
            return np.zeros(256, dtype=np.float32)
    
    def compare_embeddings_2048d(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """
        Compare two 2048D embeddings using cosine similarity
        """
        try:
            # Ensure same length
            min_len = min(len(embedding1), len(embedding2))
            emb1 = embedding1[:min_len]
            emb2 = embedding2[:min_len]
            
            # Calculate cosine similarity
            similarity = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]
            
            # Convert to distance (lower = more similar)
            distance = 1 - similarity
            return distance
            
        except Exception as e:
            print(f"‚ö†Ô∏è 2048D comparison failed: {e}")
            return 1.0

# Global instance
ultimate_2048d_detector = Ultimate2048DDetector()

def get_ultimate_2048d_embeddings(image_path: str) -> List[np.ndarray]:
    """
    Get 2048D face embeddings using the ultimate detector
    """
    return ultimate_2048d_detector.detect_faces_2048d(image_path)

def compare_embeddings_ultimate_2048d(embedding1: np.ndarray, embedding2: np.ndarray) -> float:
    """
    Compare 2048D embeddings using the ultimate detector
    """
    return ultimate_2048d_detector.compare_embeddings_2048d(embedding1, embedding2)

# Test function
def test_ultimate_2048d():
    """Test the ultimate 2048D detector"""
    print("üß™ Testing ULTIMATE 2048D DETECTOR...")
    
    # Test with a sample image if available
    test_images = []
    if os.path.exists('storage/data'):
        for root, dirs, files in os.walk('storage/data'):
            for file in files:
                if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                    test_images.append(os.path.join(root, file))
                    break
            if test_images:
                break
    
    if test_images:
        test_image = test_images[0]
        print(f"   Testing with: {test_image}")
        
        embeddings = get_ultimate_2048d_embeddings(test_image)
        print(f"   Generated {len(embeddings)} embeddings")
        
        if embeddings:
            print(f"   Embedding dimension: {len(embeddings[0])}")
            print(f"   Embedding sample: {embeddings[0][:10]}...")
            
            # Test comparison
            if len(embeddings) > 1:
                similarity = compare_embeddings_ultimate_2048d(embeddings[0], embeddings[1])
                print(f"   Similarity test: {similarity}")
        
        print("‚úÖ Ultimate 2048D detector test completed!")
    else:
        print("‚ö†Ô∏è No test images found")

if __name__ == "__main__":
    test_ultimate_2048d()
